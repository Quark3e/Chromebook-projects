
Get events info

First idea: Study lightning strike on ground with marx generator on a piece of animal tissue
above dirt.

Idea rejected because of "use of animal tissue" (even though another classmate got to use
animal tissue for their biology project)

Whilst thinking of another idea, a core teacher recommend robotics because
they have knowledge that i can write working code(program) decently.
- I ask for more info.
- they suggest something moving
- I say it won't be possible in given time frame without a better computer/processing unit
  and a 3d printer.
- Teacher says if the school gets to keep the robot and i give an example of printers with different
  price ranges they can buy a 3d printer and other supplies
- I agree but ask further what my budget will be for the parts since my class has a restricted budget
- Teacher says I'll have a separate budget since the school will get to keep it, though it can't
  be overly expensive.
- I give a quick list of 3d printers to gauge my budget price range.
- Teacher says sure and to send him the list
- I send list within a week. They respond couple days later with sure he'll arrange it.

I work on my robot project plan. Because the core topic has to circle either Math, Chem, Physics i decided
to study haptic movement optimization with comparisons between real-world output and theoretical output(forward kinematics)





=======================================
Technical storyline

Tracking:
Method: Colour(HSV(Hue, Saturation, Value/Brightness) green ball)_direct: Accelerometer_wired through arduino, to RPi via serialCOM
    End-effector's input XY axis coordinate is linearly coupled with tracked objects xy-position on webcam image feed.
    Z position is relative to area by solving area of object at different distances(z-value) from camera.
    End-effector tilt(pitch, roll) is same as accelerometer solved tilt(pitch, roll) which is received from an accelerometer
    to an arduino board(that reads raw G values and solves tilt) that then sends tilt values to the Raspberry Pi program via serial communication through usb.
Problems:
1.  Axis values become less accurate to real world the further away your object is from the camera(XY position-wise) even
    if the z-distance from ground to object is the same. Because distance from gorund doesn't change but distance to camera
    change so visible area changes which reads as object being further away. Also z-distance varies wildly the further away the object goes in any axis from the camera because HSV values
    change drastically because of change in light cast on the object
2.  Trackable area changes with varying light levels because different light gives different HSV values on the object. Even with
    tracking a wider range of acceptable values you either run the risk of not having it wide enough or you track surrounding objects
    (often a result of too big "Hue" or "Value" values).
3.  Big delay/lag: Waiting to receive tilt values from the arduino caused a significant delay which affected iteration speed of the program loop(FPS) and caused
    robot hand movement to be laggy/non-fluid.

Modification 1: Switch from tilt via serialCOM from arduino to directly receive values from acccelerometer to RPi board via i2c wire.
Pros:   Reduced arduino lag (didn't do it before because i didn't know how to and didnt have time to figure it out).
Cons:   No real new problem from this.

Modification 2: Change from HSV tracking a green ball to near-IR tracking an near-IR LED(with diffuser) attached to accelerometer via a hand palm-mount, tracked by a modified webcam(ir-filter removal mod).
Pros:   Increased tracking accuracy because nothing in the current vicinity emitted same IR values as 50mA through a SFH4546 5mm 950nm near IR LED.
Cons:   Complicated 2d projection from the 3d model/shape of the IR tracking object because of diffusors: Because the led's only had
        a 20 degree range, five were used(one for each axis direction(two per axis) except z which only had one) to accomodate for this but even then there were dead-zones without
        any significant trackable light, therefore foam squares were glued onto the led's to diffuse the light more. This worked well, but caused the light emitting shape to be "absurd".
        To solve z-axis, a polynomial equation was generated by measuring 2d-projection area at varying (manually input) z-distances to the camera. This equation worked well when object was
        moving along the same z-axis as the camera without tilt, but if the object moved in XY directions or changed tilt then the equation was only good as a speculation.

Modification 3: Generated/Mapped-out and used a full tilt-area dataset for solving correct z-axis value from given 2d-projection area and tilt values.
Pros:   Gave a better/more-correct z-axis result because now the distance solving also included tilt.
Cons:   Took a long time for decent improvement. In order to map out every single tilt-area case/point(for given resolution) so that it can be used as a table, I had to record every
        possible(because the tilt resolution wasn't that stable(in exchange for speed)) tilt-area data point, for each axis between 100-400(limit relative to robots work envelope). Theoretically
        that's 180*180*300 or 9'720'000 individual points which I wasn't gonna do by hand so i recorded a decent amount of points by hand then used second degree polynomial regression analysis tool by
        Numpy module in python. This was of course not too accurate but was far better than manually recording each point with my hand in the air, moving slightly, 9 million times with a diy script that took
        0.1 seconds to record and save a single point (took that look because i needed a display window via opencv to make sure my hand didn't move too far). Are you crazy? Do you know how long i had to hold my hand up
        in order to record 200 points alone?? I'd burn the data and learn matrices than do this crap.
        Is knot dat accurate.

Modification 4: Used a nodemcu esp8266 board to wirelessly send tilt values from accelerometer to RPi board/program via UDP protocol over a port.
Pros:   No longer had to have a wire strapped to the tracking object. Gave a lot more freedom to move without risk of tangling or ripping out the cable.
        Gave an easier way of powering objects tracked IR-leds since now they can be connected to the nodemcu boards V_out pin which is attached to a power bank
        Didn't give a noticeable delay to the system either.
Cons:   Complicated connecting to the rpi a bit. Made hand piece heavier as now it carried a board and a powerbank.

Modification 5: Change axis-solving by using/tracking-from two cameras at different locations and tilt and analytic geometry.
Pros:   Made z-axis solving a lot more simple and accurate because now there was a set/accurate way to solve z-axis value through basic trigonometry.
        Gave more accurate z and x axis results (because the two cameras were placed along the same y-axis along different y-value).
Cons:   Slowed down main-program iteration speed because now it had to process two camera feeds before giving an output, compared to the single camera before.
        Axis movement is restricted the closer the tracked-object is to the cameras because the object has to be visible in both cameras to be tracked, and each camera
        only has a FOV of 60 degrees, and one of the cameras has a slight tilt meaning the crossed FOV volume is slightly less.
    
Modification 6: Run image processing in parallel threads: As in run both cameras processing their camera feed in parallel,
                whilst they're running concurrently with the main loop(that reads new images, solves position and controls robot). This means that the two cam feed processing are in parallel, but that parallel
                process is concurrent with the main loop.
                Theoretical time is main_process+cam0_thread_process whereas before it was main_process+cam0_thread_process+cam1_thread_process
Pros:   Reduced image processing lag to that of a single image processing process (~40ms on rpi4B overclocked to 2.2GHz) (Â±thread initialising and closing time).
Cons:   No cons really. The Raspberry Pi 4B board is fairly good at multiprocessing in parallel across its 4 cores so running processes in separate threads on other cores
        is fairly good as it doesn't affect the "main"(where main-loop is running) core.

Modification 7: Run image receiving and processing in dedicated cores throughout the program from start to finish and communicate the tracked cam positions via mutex objects.
Pros:   Sped up image processing by a lot as now the update time in the main loop is that of itself updating new variables and doing trigonometry and sending, which is ~1ms.
Cons:   Risk of non-sync data. Because each camera processing thread is separate from other two threads(other cam process / main loop thread) and a camera processing thread is fairly slow (~30-40ms) there's a risk
        of one camera processing a frame and output it, whilst the other camera lags behind by just having captured a frame. This is is usually fine as the lag-position differences are marginal but if in that split difference a big
        axis-movement of the tracked object was performed then the output cam positions can vary enough for the analytic geometry formula to output an extreme spike in an axis. It'll likely solve itself in a couple milliseconds
        but still, that risk has happened on occasion.

